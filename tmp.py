########## 这个地方用于放置以后会用到的代码 ##########
########## 这个地方用于放置以后会用到的代码 ##########
########## 这个地方用于放置以后会用到的代码 ##########

# 这个函数用于在数据集文本中添加 R token 占位符
def add_r_token_placeholders(text, image_token_end="<|vision_end|>"):
    # 定义你的特殊 token
    start_token = "<|text_R_token|>"
    end_token = "<|text_R_token_end|>"
    # 核心：生成 40 个占位符
    placeholders = "<|text_R_token_placeholder|>" * 40#替换成真实的trtoken长度

    # 构造完整的注入串： <Start> [40个坑] <End>
    injection_str = f"{start_token}{placeholders}{end_token}"

    # 强制插入到图片结束符后面
    # 注意：这里假设你的文本里已经有了 image 相关的 token
    if image_token_end in text:
        text = text.replace(image_token_end, image_token_end + injection_str)

    return text
#todo：1、局部图在全局图中的位置如何表示？局部图的顺序又如何表示？
#todo：2、坐标框越界、畸形、过小，直接让模型说看不清
#todo：3、KV Cache 优化？使用二阶段重启，把<|request_zoom|>视作文本结束，然后无需用户输入直接再开一轮对话。
#todo：4、禁止递归缩放
#todo：5、针对模型自主触发的缩放，设计“终止-插入-继续”的状态机，确保生成流不被截断

# ==============================================================================
# Workflow Scenarios & Roadmap (L1 - L6)
# ==============================================================================

#TODO: L1 (最简通路 - 纯指令模式):
#场景: 用户同时输入“坐标框”和“<|request_zoom|>”指令。
#处理: 系统在前处理阶段直接拦截，执行剪裁、缩放、压缩，送入二阶段推理。
#状态: [核心功能，已实现]
#TODO: L2 (半自动 - 仅定位模式):
#场景: 用户仅给出文本指令（如“把图里的猫框出来”），未要求细看。
#处理: 模型仅输出 Grounding 坐标，不输出 <|request_zoom|>，不触发后续剪裁流程。
#状态: [模型原生能力，无需额外开发]
#TODO: L3 (条件触发 - 仅判断模式):
#场景: 用户输入了坐标框，询问框内内容，但未显式要求放大。
#处理: 由模型根据 Context 自主判断是否需要输出 <|request_zoom|>。
#策略: 4B 模型建议在 System Prompt 中 Bias 向“只要有框且问细节，就倾向于放大”，或直接硬编码为强制放大。
#TODO: L4 (模拟全自动 - 关键词触发模式):
#场景: 用户仅输入文本（如“放大细看那辆车”），不提供坐标。
#处理: 利用 SFT 训练模型对“放大细看”等 Trigger Word 产生反应 -> 模型执行 Grounding 输出坐标 -> 模型追加 <|request_zoom|>。
#策略: 替代纯隐式推理，将“意图识别”降级为“指令跟随 + 视觉定位”。
#TODO: L5 (多点并发 - 比较/列表模式):
#场景: 用户要求对比多处细节，模型一次性输出多个坐标框和 <|request_zoom|>。
#处理: 后端需支持 Batch Crop 和 Multi-Image Token 插入，注意 Token 长度预算管理。
#难点: 多个局部图在 Prompt 中的插入位置（通常追加在最后）及对应的 Index 标识。
#TODO: L6 (交互修正 - 空间推理模式 - 4B模型降级处理):
#场景: AI 画框不准，用户口语修正（“往左一点”）。
#处理: 鉴于 4B 模型空间推理能力有限，此处不尝试计算新坐标。
#策略: 训练模型学会“拒绝”或“回退”，例如引导用户：“请您直接帮我框选一下准确位置”。
